---
title: "Practical Machine Learning Course Project"
author: "David H"
date: "22/12/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Prediction of Weight Lifting Exercise Quality
#### Background
Over the last few years, there has been a massive increase in the no of gym users using wearable devices such as Fitbit, Jawbone Up and Nike FuelBand. Typically, these are used to quantify the amount of a particular activity done. Rarely, has analysis been done into the quality of the exercise. Using data from accelerometers on the belt, forearm, arm and dumbell, 6 different participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The source of this data is taken from:  http://groupware.les.inf.puc-rio.br/har .

#### Aim
The aim of this project is to build a model that uses the measurements from the accelerometers and classify which of the 5 different ways weight lifting was performed. For reference, outcome class A represents the "correct"" way where B,C,D and E classes correspond to 4 common mistakes. 

#### Data
The training data on which the classification model was built is stored here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The validation data contains 20 validation cases without outcome is stored here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Both of these csv files were imported into R and some processing performed. The outcome variable classe and user name were converted to factors. Also, the accelerometer data in columns 7 through to 159 were explicitly converted to numeric values.

```{r warning=FALSE}
setwd("~/Dropbox/Coursera/Practical Machine Learning/Course Project")
input<- read.csv(file="pml-training.csv",stringsAsFactors = FALSE)
input$classe<- factor(input$classe)
input$user_name<- factor(input$user_name)
input[,7:159]<- apply(input[,7:159], 2, as.numeric)
```

The input dataset contains `r nrow(input)` rows and `r ncol(input)` columns. 

It was decided to split this input data into training and testing datasets using a 75%/25% split. The first 7 columns were removed as these contain unnecessary information such as row no, user name and various date timestamps. Additionally, it was necessary to remove approx half of the columns due to near zero variance. These were overwhelmingly populated with NA values. 

```{r warning=FALSE, message=FALSE}
library(caret)
inTrain<- createDataPartition(y=input$classe, p=0.75, list=FALSE)
training<- input[inTrain,colSums(is.na(input))==0]
testing<- input[-inTrain,colSums(is.na(input))==0]
training<- training[,-c(1:7)]
testing<- testing[,-c(1:7)]
```

The training and test datasets both have `r ncol(training)` columns and `r nrow(training)` and `r nrow(testing)` rows respectively. As a final check the near zero variance check was run again confirming each remaining column had sufficient uniqueness.

```{r warning=FALSE}
nzv<- nearZeroVar(training,saveMetrics = TRUE)
head(nzv)
```

#### Initial analysis
Some initial analysis was done on the training dataset. Indications were that there were an approximately even spread of 5 output classes and that the accelerometer data tended to cluster around particular values.

```{r warning=FALSE}
table(training$classe)
pairs(training[,c(1:5)])
```

Given the categorical nature of outcome 'classe', it was determined to build a decision tree using caret package.

```{r warning=FALSE, message=FALSE}
set.seed(123)
modFit1<- train(classe~.,method="rpart",data=training)
modFit1
modFit1$finalModel
```

With the default parameters in caret, it can be seen that the final model was created by bootstrapping with 25 samples. This cross validation ensures that parameters can be tuned and the testing dataset alone can be used to independently determine accuracy/out of sample error. 

```{r warning=FALSE}
confusionMatrix(predict(modFit1,testing),testing$classe)
```

The accuracy of the testing dataset is ~48% which is a lot poorer than expected. Part of this may be due to the decision tree not being able to predict class D outcomes. To get around this, it was determined to build many different decision trees on sub samples of training data and majority vote the prediction. This is the premise behind Random Forests.

#### Final model

Due to the slowness of running Random Forests using caret default settings, it was decided to switch to the underlying package: randomForest. 

```{r warning=FALSE, message=FALSE}
library(randomForest)
set.seed(123)
modFit1<- randomForest(classe~.,data=training)
modFit1
```

This indicates that 500 different decision trees were built with each decison tree built on sampling approx 2/3 of the training data with replacement. A random sample of 7 predictors was then used to determine each best split. For each tree, the out-of-bag(OOB) error rate was estimated on the remaining 1/3 of data not used. This was then averaged across all 500 decision trees and is similar to other forms of cross validation. 

For the independent determination of model accuracy, the model was applied to the testing dataset to give a 99.6% accuracy. 

```{r warning=FALSE}
confusionMatrix(predict(modFit1,testing),testing$classe)
```

Also, plotting the model indicates the decreasing OOB error rate as the no of decision trees increases. The black line plateaus at ~100-150 trees and so the model is optimised. 

```{r warning=FALSE}
plot(modFit1)
```

The most important variables across the 500 decision trees can be shown in order of largest mean decreasing gini if they were excluded. Notably variables such as roll_belt and pitch_forearm also featured highly in the single decision tree.

```{r warning=FALSE}
varImpPlot(modFit1)
```

As a final step the 20 validation cases were read in and the final model used to predict the outcomes, of which all matched.

```{r warning=FALSE}
setwd("~/Dropbox/Coursera/Practical Machine Learning/Course Project")
validation<- read.csv(file="pml-testing.csv",stringsAsFactors = FALSE)
validation$user_name<- factor(validation$user_name)
validation[,7:159]<- apply(validation[,7:159], 2, as.numeric)
predict(modFit1,validation)
```

This final model is accurate enough to classify between the 5 different ways the exercise is conducted.  
